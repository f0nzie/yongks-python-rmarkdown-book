
# pandas

```{r include=FALSE}
library(reticulate)
use_condaenv('Anaconda3')    #conda_list() - to find out the name of conda environment
```

```{python include=FALSE, results='hide'}
import numpy as np
import pandas as pd
from IPython.core.display import display, HTML
display(HTML("<style>.container { width:75% !important; margin-left:350px; }</style>"))
pd.set_option( 'display.notebook_repr_html', False)  # render Series and DataFrame as text, not HTML
pd.set_option( 'display.max_column', 10)    # number of columns
pd.set_option( 'display.max_rows', 10)      # number of rows
pd.set_option( 'display.width', 90)         # number of characters per row
```

## Modules Import

```{python}
import pandas as pd

## Other Libraries
import numpy as np
import datetime as dt
from datetime import datetime
from datetime import date
```

## Pandas Objects

### Pandas Data Types
- pandas.Timestamp
- pandas.Timedelta
- pandas.Period
- pandas.Interval
- pandas.DateTimeIndex

### Pandas Data Structure  

|Type        | Dimension | Size      | Value   | Constructor
|:---------- |:----------|:----------|:--------|:----------------------------------------------------
|Series      | 1         | Immutable | Mutable | pandas.DataFrame( data, index, dtype, copy)  
|DataFrame   | 2         | Mutable   | Mutable | pandas.DataFrame( data, index, columns, dtype, copy)
|Panel       | 3         | Mutable   | Mutable | 

**data** can be ndarray, list, constants  
**index** must be unique and same length as data. Can be integer or string
**dtype** if none, it will be inferred  
**copy** copy data. Default false

## Class Method

### Creating Timestamp Objects

Pandas **`to_datetime()`** can:  
- Convert list of dates to **DateTimeIndex**  
- Convert list of dates to **Series of Timestamps**  
- Convert single date into **Timestamp** Object
. Source can be **string, date, datetime object**

#### From List to `DateTimeIndex`

```{python}
dti = pd.to_datetime(['2011-01-03',             # from string
                       date(2018,4,13),         # from date
                       datetime(2018,3,1,7,30)] # from datetime
              )
print( dti,
      '\nObject Type:  ', type(dti),
      '\nObject dtype: ', dti.dtype,
      '\nElement Type: ', type(dti[1]))
```

#### From List to Series of Timestamps

```{python}
sdt = pd.to_datetime(pd.Series(['2011-01-03',      # from string
                                date(2018,4,13),        # from date
                                datetime(2018,3,1,7,30)]# from datetime
              ))
print(sdt,
      '\nObject Type:  ',type(sdt),
      '\nObject dtype: ', sdt.dtype,
      '\nElement Type: ',type(sdt[1]))
```

#### From Scalar to Timestamp

```{python}
print( pd.to_datetime('2011-01-03'), '\n',
       pd.to_datetime(date(2011,1,3)), '\n',
       pd.to_datetime(datetime(2011,1,3,5,30)), '\n',
       '\nElement Type: ', type(pd.to_datetime(datetime(2011,1,3,5,30))))
```

### Generate Timestamp Sequence

The function `date_range()` return **`DateTimeIndex`** object. Use `Series()` to convert into Series if desired.

#### Hourly

If start time not specified, default to 00:00:00.  
If start time specified, it will be honored on all subsequent Timestamp elements.  
Specify **start** and **end**, sequence will automatically distribute Timestamp according to **frequency**.  

```{python}
print(
  pd.date_range('2018-01-01', periods=3, freq='H'),
  pd.date_range(datetime(2018,1,1,12,30), periods=3, freq='H'),
  pd.date_range(start='2018-01-03-1230', end='2018-01-03-18:30', freq='H'))
```

#### Daily

When the **frequency is Day and time is not specified**, output is date distributed.  
When time is specified, output will honor the time.

```{python}
print(
  pd.date_range(date(2018,1,2), periods=3, freq='D'),
  pd.date_range('2018-01-01-1230', periods=4, freq='D'))
```

#### First Day Of Month

Use `freq=MS`, M stands for montly, S stand for Start. If the **day** specified, the sequence start from first day of following month.

```{python, jupyter_meta = list(scrolled = TRUE)}
print(
  pd.date_range('2018-01', periods=4, freq='MS'),
  pd.date_range('2018-01-09', periods=4, freq='MS'),
  pd.date_range('2018-01-09 12:30:00', periods=4, freq='MS') )
```
#### Last Day of Month

Sequence always starts from the end of the specified month.

```{python}
print(
  pd.date_range('2018-01', periods=4, freq='M'),
  pd.date_range('2018-01-09', periods=4, freq='M'),
  pd.date_range('2018-01-09 12:30:00', periods=4, freq='M'))
```

### Frequency Table (crosstab)

crosstab returns **Dataframe** Object
```
crosstab( index = <SeriesObj>, columns = <new_colName> )                # one dimension table
crosstab( index = <SeriesObj>, columns = <SeriesObj> )                  # two dimension table
crosstab( index = <SeriesObj>, columns = [<SeriesObj1>, <SeriesObj2>] ) # multi dimension table   
crosstab( index = <SeriesObj>, columns = <SeriesObj>, margines=True )   # add column and row margins
```

#### Sample Data

```{python}
n = 200
comp = ['C' + i for i in np.random.randint( 1,4, size  = n).astype(str)] # 3x Company
dept = ['D' + i for i in np.random.randint( 1,6, size  = n).astype(str)] # 5x Department
grp =  ['G' + i for i in np.random.randint( 1,3, size  = n).astype(str)] # 2x Groups
value1 = np.random.normal( loc=50 , scale=5 , size = n)
value2 = np.random.normal( loc=20 , scale=3 , size = n)
value3 = np.random.normal( loc=5 , scale=30 , size = n)

mydf = pd.DataFrame({
    'comp':comp, 
    'dept':dept, 
    'grp': grp,
    'value1':value1, 
    'value2':value2,
    'value3':value3 })
mydf.head()
```

#### One DimensionTable

```{python}
## Frequency Countn For Company, Department
print(
  pd.crosstab(index=mydf.comp, columns='counter'),'\n\n',
  pd.crosstab(index=mydf.dept, columns='counter'))
```

#### Two Dimension Table

```{python}
pd.crosstab(index=mydf.comp, columns=mydf.dept)
```

#### Higher Dimension Table

Crosstab header is **multi-levels index** when more than one column specified.

```{python}
tb = pd.crosstab(index=mydf.comp, columns=[mydf.dept, mydf.grp])
print( tb, '\n\n',
       tb.columns )
```

Select **sub-dataframe** using multi-level referencing.

```{python}
print( 'Under D2:\n', tb['D2'], '\n\n',
       'Under D2-G2:\n',tb['D2','G1'])
```

#### Getting Margin
Extend the crosstab with 'margin=True' to have sum of rows/columns, presented in **new column/row named 'All'**.

```{python}
tb = pd.crosstab(index=mydf.dept, columns=mydf.grp, margins=True)
tb
```

```{python}
print(
  'Row Sums:     \n', tb.loc[:,'All'],
  '\n\nColumn Sums:\n', tb.loc['All'])
```

#### Getting Proportion
Use matrix operation divide each row with its respective column sum.

```{python}
tb/tb.loc['All']
```

### Concatination

#### Sample Data

```{python}
s1 = pd.Series(['A1','A2','A3','A4'])
s2 = pd.Series(['B1','B2','B3','B4'], name='B')
s3 = pd.Series(['C1','C2','C3','C4'], name='C')
df
```

#### Column-Wise

**Combinining Multiple Series Into A New DataFrame**  
- Added series will have 0,1,2,... column names (if Series are not named originally)  
- None series will be ignored  
- `axis=1` means column-wise

```{python}
pd.concat([s1,s2,s3, None], axis=1)
```

**Add Multiple Series Into An Existing DataFrame**  
- No change to original data frame column name  
- Added columns from series will have 0,1,2,3,.. column name

```{python}
df = pd.DataFrame({ 'A': s1, 'B': s2})
pd.concat([df,s3,s1, None],axis=1)
```

#### Row-Wise

```{python}

```


### External Data

#### `html_table` Parser

This method require **html5lib** library.  
- Read the web page, create a list: which contain one or more dataframes that maps to each html table found  
- Scrap all detectable html tables  
- Auto detect column header  
- Auto create index using number starting from 0  

```
read_html(url)  # return list of dataframe(s) that maps to web table(s) structure
```

```{python}
df_list = pd.read_html('https://www.malaysiastock.biz/Listed-Companies.aspx?type=S&s1=18')  ## read all tables
df = df_list[6]  ## get the specific table

print ('Total Table(s) Found : ', len(df_list), '\n',
       'First Table Found:      ',df)
```

#### CSV Writing

**Syntax**  

```
DataFrame.to_csv(
  path_or_buf=None,   ## if not provided, result is returned as string
  sep=', ', 
  na_rep='', 
  float_format=None, 
  columns=None,       ## list of columns name to write, if not provided, all columns are written
  header=True,        ## write out column names
  index=True,         ## write row label
  index_label=None, 
  mode='w', 
  encoding=None,      ## if not provided, default to 'utf-8'
  quoting=None, quotechar='"', 
  line_terminator=None, 
  chunksize=None, 
  date_format=None, 
  doublequote=True, 
  escapechar=None, 
  decimal='.')

```

Example below shows column value containing different special character. Note that pandas handles these very well by default.

```{python}
mydf = pd.DataFrame({'Id':[10,20,30,40], 
                     'Name':  ['Aaa','Bbb','Ccc','Ddd'],
                     'Funny': ["world's most \clever", 
                     "Bloody, damn, good", 
                     "many\nmany\nline", 
                     'Quoting "is" tough']})
mydf.set_index('Id', inplace=True)
mydf.to_csv('data/csv_test.csv', index=True)
mydf
```

**This is the file saved**

```{r}
system('more data\\csv_test.csv')
```

**All content retained when reading back by Pandas**

```{python}
pd.read_csv('data/csv_test.csv', index_col='Id')
```

#### CSV Reading

**Syntax**  

```
pandas.read_csv( 
    'url or filePath',                     # path to file or url 
    encoding    = 'utf_8',                 # optional: default is 'utf_8'
    index_col   = ['colName1', ...],       # optional: specify one or more index column
    parse_dates = ['dateCol1', ...],       # optional: specify multiple string column to convert to date
    na_values   = ['.','na','NA','N/A'],   # optional: values that is considered NA
    names       = ['newColName1', ... ],   # optional: overwrite column names
    thousands   = '.',                     # optional: thousand seperator symbol
    nrows       = n,                       # optional: load only first n rows
    skiprows    = 0,                       # optional: don't load first n rows
    parse_dates = False,                   # List of date column names
    infer_datetime_format = False          # automatically parse dates
)
```
Refer to full codec [Python Codec](https://docs.python.org/3/library/codecs.html#standard-encodings).

**Default Import**  

- index is sequence of integer 0,1,2...   
- only two data types detection; **number (float64/int64) and string (object)**  
- **date is not parsed**, hence stayed as string  

```{python}
goo = pd.read_csv('data/goog.csv', encoding='utf_8')
print(goo.head(), '\n\n',
      goo.info())
```

**Specify Data Types**  

- To customize the data type, use **`dtype`** parameter with a **dict** of definition.  

```{python}
d_types = {'Volume': str}
pd.read_csv('data/goog.csv', dtype=d_types).info()
```

**Parse Datetime**

You can specify multiple date-alike column for parsing

```{python}
pd.read_csv('data/goog.csv', parse_dates=['Date']).info()
```

**Parse Datetime, Then Set as Index**  
- Specify names of date column in `parse_dates=`  
- When date is set as index, the type is **`DateTimeIndex`**  

```{python}
goo3 = pd.read_csv('data/goog.csv',index_col='Date', parse_dates=['Date'])
goo3.info()
```

### Inspection

#### Structure `info`

**info()** is a function that print information to screen. It doesn't return any object

```
dataframe.info()  # display columns and number of rows (that has no missing data)
```

```{python, jupyter_meta = list(hidden = TRUE)}
goo.info()
```

#### `head`

```{python, jupyter_meta = list(hidden = TRUE)}
goo.head()
```

## class: Timestamp

This is an enhanced version to datetime standard library.  
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Timestamp.html#pandas.Timestamp

### Constructor

#### From Number

```{python}
print( pd.Timestamp(year=2017, month=1, day=1),'\n',  #date-like numbers
       pd.Timestamp(2017,1,1), '\n',                  # date-like numbers
       pd.Timestamp(2017,12,11,5,45),'\n',            # datetime-like numbers
       pd.Timestamp(2017,12,11,5,45,55,999),'\n',     # + microseconds
       pd.Timestamp(2017,12,11,5,45,55,999,8),'\n',   # + nanoseconds
       type(pd.Timestamp(2017,12,11,5,45,55,999,8)),'\n')
```

#### From String

Observe that pandas support many string input format  

**Year Month Day**, default has no timezone
```{python}
print( pd.Timestamp('2017-12-11'),'\n',   # date-like string: year-month-day
       pd.Timestamp('2017 12 11'),'\n',   # date-like string: year-month-day
       pd.Timestamp('2017 Dec 11'),'\n',  # date-like string: year-month-day
       pd.Timestamp('Dec 11, 2017'))      # date-like string: year-month-day
```

**YMD Hour Minute Second Ms**

```{python}
print( pd.Timestamp('2017-12-11 0545'),'\n',     ## hour minute
       pd.Timestamp('2017-12-11-05:45'),'\n',
       pd.Timestamp('2017-12-11T0545'),'\n',
       pd.Timestamp('2017-12-11 054533'),'\n',   ## hour minute seconds
       pd.Timestamp('2017-12-11 05:45:33'))
```

**With Timezone** can be included in various ways.

```{python}
print( pd.Timestamp('2017-01-01T0545Z'),'\n',  # GMT 
       pd.Timestamp('2017-01-01T0545+9'),'\n', # GMT+9
       pd.Timestamp('2017-01-01T0545+0800'),'\n',   # GMT+0800
       pd.Timestamp('2017-01-01 0545', tz='Asia/Singapore'),'\n')
```

#### From Standard Library ```datetime``` and ```date``` Object

```{python}
print( pd.Timestamp(date(2017,3,5)),'\n',           # from date
       pd.Timestamp(datetime(2017,3,5,4,30)),'\n',  # from datetime
       pd.Timestamp(datetime(2017,3,5,4,30), tz='Asia/Kuala_Lumpur')) # from datetime, + tz
```

### Attributes

We can tell many things about a Timestamp object.

```{python}
ts = pd.Timestamp('2017-01-01T054533+0800') # GMT+0800
print( ts.month, '\n',
       ts.day, '\n',
       ts.year, '\n',
       ts.hour, '\n',
       ts.minute, '\n',
       ts.second, '\n',
       ts.microsecond, '\n',
       ts.nanosecond, '\n',
       ts.tz, '\n',
       ts.daysinmonth,'\n',
       ts.dayofyear, '\n',
       ts.is_leap_year, '\n',
       ts.is_month_end, '\n',
       ts.is_month_start, '\n',
       ts.dayofweek)
```

Note that timezone (tz) is a **pytz object**.

```{python}
ts1 = pd.Timestamp(datetime(2017,3,5,4,30), tz='Asia/Kuala_Lumpur')   # from datetime, + tz
ts2 = pd.Timestamp('2017-01-01T054533+0800') # GMT+0800
ts3 = pd.Timestamp('2017-01-01T0545')

print( ts1.tz, 'Type:', type(ts1.tz), '\n',
       ts2.tz, 'Type:', type(ts2.tz), '\n',
       ts3.tz, 'Type:', type(ts3.tz)  )
```

### Instance Methods

#### Atribute-like Methods

```{python}
ts = pd.Timestamp(2017,1,1)
print( ' Weekday:    ', ts.weekday(), '\n',
       'ISO Weekday:',  ts.isoweekday(), '\n',
       'Day Name:   ',  ts.day_name(), '\n',
       'ISO Calendar:',  ts.isocalendar()
       )
```

#### Timezones

**Adding Timezones and Clock Shifting**  

- `tz_localize` will add the timezone, however will not shift the clock.  
- Once a timestamp had gotten a timezone, you can easily shift the clock to another timezone using `tz_convert()`  

```{python}
ts = pd.Timestamp(2017,1,10,10,34)        ## No timezone
ts1 = ts.tz_localize('Asia/Kuala_Lumpur')  ## Add timezone
ts2 = ts1.tz_convert('UTC')                 ## Convert timezone
print(' Origininal Timestamp           :', ts,  '\n',
      'Loacalized Timestamp (added TZ):', ts1, '\n',
      'Converted Timestamp (shifted)  :',ts2)
```

**Removing Timezone**

Just apply **None with `tz_localize`** to remove TZ infomration.

```{python}
ts = pd.Timestamp(2017,1,10,10,34)        ## No timezone
ts = ts.tz_localize('Asia/Kuala_Lumpur')  ## Add timezone
ts = ts.tz_localize(None)                 ## Convert timezone
ts
```

#### Formatting 

**`strftime`**  

Use **```strftime()```** to customize string format. For complete directive, see below: https://docs.python.org/3/library/datetime.html#strftime-strptime-behavior

```{python}
ts = pd.Timestamp(2017,1,10,10,34)        ## No timezone
ts = ts.tz_localize('Asia/Kuala_Lumpur')  ## Add timezone
ts.strftime("%m/%d")
```

**`isoformat`**  

Use **```isoformat()```** to format ISO string (**without timezone**)

```{python}
ts = pd.Timestamp(2017,1,10,10,34)        
ts1 = ts.tz_localize('Asia/Kuala_Lumpur') 
print( ' ISO Format without TZ:', ts.isoformat(), '\n',
       'ISO Format with TZ   :', ts1.isoformat())
```

#### Type Conversion

**Convert To `datetime.datetime/date`**

Use `to_pydatetime()` to convert into standard library **`datetime.datetime`**.  From the 'datetime' object, apply `date()` to get **`datetime.date`**

```{python}
ts = pd.Timestamp(2017,1,10,7,30,52)
print(
  'Datetime:',  ts.to_pydatetime(), '\n',
  'Date Only:', ts.to_pydatetime().date())
```

**Convert To `numpy.datetime64`**

Use `to_datetime64()` to convert into ```numpy.datetime64```

```{python}
ts = pd.Timestamp(2017,1,10,7,30,52)
ts.to_datetime64()
```


#### ```ceil```

```{python}
print( ts.ceil(freq='D') ) # ceiling to day
```

#### Updating

`replace()`

```{python}
ts.replace(year=2000, month=1,day=1)
```

## class: DateTimeIndex

### Creating 

Refer to Pandas class method above.

### Instance Method

#### Data Type Conversion

**Convert To datetime.datetime**  
Use **```to_pydatetime```** to convert into python standard **datetime.datetime** object

```{python}
print('Converted to List:', dti.to_pydatetime(), '\n\n',
      'Converted Type:',    type(dti.to_pydatetime()))
```

#### Structure Conversion

**Convert To Series: `to_series`**  
This creates a Series where **index and data** with the same value

```{python}
#dti = pd.date_range('2018-02', periods=4, freq='M')
dti.to_series()
```

**Convert To DataFrame: `to_frame()`**  
This convert to **single column DataFrame** with index as the same value

```{python}
dti.to_frame()
```

### Attributes

**All Timestamp Attributes** can be used upon DateTimeIndex.

```{python}
print( dti.weekday, '\n',
       dti.month, '\n',
       dti.daysinmonth)
```

## class: Series

Series allows different data types (object class) as its element

```
pandas.Series(data=None, index=None, dtype=None, name=None, copy=False, fastpath=False)
- data array-like, iterable, dict or scalar
- If dtype not specified, it will infer from data.
```

### Constructor

#### Empty Series

Passing no data to constructor will result in empty series. By default, empty series dtype is float.

```{python}
s = pd.Series(dtype='object')
print (s, '\n',
       type(s))
```

#### From Scalar

If data is a scalar value, an **index must be provided**. The **value will be repeated** to match the length of index

```{python}
pd.Series( 99, index = ['a','b','c','d'])
```

#### From array-like

**From list**

```{python}
pd.Series(['a','b','c','d','e'])           # from Python list
```


**From numpy.array**  
If index is not specified, default to 0 and continue incrementally

```{python}
pd.Series(np.array(['a','b','c','d','e']))
```

**From DateTimeIndex**

```{python}
pd.Series(pd.date_range('2011-1-1','2011-1-3'))
```


#### From Dictionary
The **dictionary key** will be the index. Order is **not sorted**.

```{python}
pd.Series({'a' : 0., 'c' : 5., 'b' : 2.})
```

If **index sequence** is specifeid, then Series will forllow the index order  
Objerve that **missing data** (index without value) will be marked as NaN

```{python}
pd.Series({'a' : 0., 'c' : 1., 'b' : 2.},index = ['a','b','c','d'])
```

#### Specify Index

```{python}
pd.Series(['a','b','c','d','e'], index=[10,20,30,40,50])
```

#### Mix Element Types

dType will be **'object'** when there were mixture of classes

```{python}
ser = pd.Series(['a',1,2,3])
print('Object Type :  ', type(ser),'\n',
      'Object dType:  ', ser.dtype,'\n',
      'Element 1 Type: ',type(ser[0]),'\n',
      'Elmeent 2 Type: ',type(ser[1]))
```

#### Specify Data Types
By default, dtype is **inferred** from data.

```{python}
ser1 = pd.Series([1,2,3])
ser2 = pd.Series([1,2,3], dtype="int8")
ser3 = pd.Series([1,2,3], dtype="object")

print(' Inferred:        ',ser1.dtype, '\n',
      'Specified int8:  ',ser2.dtype, '\n',
      'Specified object:',ser3.dtype)
```

### Accessing Series

```
series     ( single/list/range_of_row_label/number ) # can cause confusion
series.loc ( single/list/range_of_row_label )
series.iloc( single/list/range_of_row_number )
```

#### Sample Data

```{python}
s = pd.Series([1,2,3,4,5],index=['a','b','c','d','e']) 
s
```

#### by Row Number(s)

**Single Item**. 
Notice that inputing a number and list of number give different result.

```{python}
print( 'Referencing by number:',s.iloc[1],'\n\n',
       '\nReferencing by list of number:\n',s.iloc[[1]])
```


**Multiple Items**

```{python}
s.iloc[[1,3]] 
```


**Range (First 3)**

```{python}
s.iloc[:3]
```


**Range (Last 3)**

```{python, jupyter_meta = list(scrolled = TRUE)}
s.iloc[-3:]
```

**Range (in between)**

```{python}
s.iloc[2:3]
```

#### by Index(es)

**Single Label**. Notice the difference referencing input: single index and list of index.  
**Warning**: if index is invalid, this will result in error.

```{python}
print( s.loc['c'], '\n',
       s[['c']])
```

**Multiple Labels**

If index is not found, it will return **NaN**

```{python}
s.loc[['k','c']]
```

** Range of Labels **

```{python}
s.loc['b':'d']
```

#### Filtering

Use **logical array** to filter

```{python}
s = pd.Series(range(1,8))
s[s<5]
```

Use **where**  
The where method is an application of the if-then idiom. For each element in the calling Series, if `cond` is True the element is used; otherwise `other` is used.

```
.where(cond, other=nan, inplace=False)
```

```{python}
print(s.where(s<4),'\n\n',
      s.where(s<4,other=None) )
```

### Updating Series

#### by Row Number(s)

```{python}
s = pd.Series(range(1,7), index=['a','b','c','d','e','f'])
s[2] = 999
s[[3,4]] = 888,777
s
```

#### by Index(es)

```{python}
s = pd.Series(range(1,7), index=['a','b','c','d','e','f'])
s['e'] = 888
s[['c','d']] = 777,888
s
```

### Series Attributes

#### The Data

```{python}
s = pd.Series([1,2,3,4,5],index=['a','b','c','d','e'],name='SuperHero') 
s
```

#### The Attributes

```{python}
print( ' Series Index:    ',s.index, '\n',
       'Series dType:    ', s.dtype, '\n',
       'Series Size:     ', s.size, '\n',
       'Series Shape:    ', s.shape, '\n',
       'Series Dimension:', s.ndim)
```

### Instance Methods

#### .reset_index ()
Resetting index will:  
- Convert index to a normal column, header is 'index'
- Index renumbered to ,1,2,3
- Retrun DataFrame (became two columns)

```{python}
print(s)
print(s.reset_index())
```

#### Structure Conversion

A series structure contain **`value`** (in numpy array), its **`dtype`** (data type of the numpy array).    
Use **```values```** to retrieve into ```numpy.ndarray``. Use **`dtype`** to understand the data type.  

```{python}
s = pd.Series([1,2,3,4,5])
print(' Series value:      ', s.values, '\n', 
      'Series value type: ', type(s.values), '\n',
      'Series dtype:      ',s.dtype)
```

Use **`pandas.Series.tolist()`** to convert into standard python ```list``

```{python}
pd.Series.tolist(s)
```

#### DataType Conversion

Use **```astype()```** to convert to another numpy supproted datatypes, results in a new Series.  
**Warning**: casting to incompatible type will result in **error**

```{python}
s.astype('int8')
```

### Series Operators

The result of applying operator (arithmetic or logic) to Series object **returns a new Series object**

#### Arithmetic Operator

```{python}
s1 = pd.Series( [100,200,300,400,500] )
s2 = pd.Series( [10, 20, 30, 40, 50] )
```

**Apply To One Series Object**

```{python}
s1 - 100
```

**Apply To Two Series Objects**

```{python}
s1 - s2
```

#### Logic Operator

- Apply logic operator to a Series return a **new Series** of boolean result  
- This can be used for **Series or DataFrame filtering**

```{python}
bs = pd.Series(range(0,10))
bs>3
```

```{python}
~((bs>3) & (bs<8) | (bs>7))
```

### Series `.str` Accesor

If the underlying data is **str** type, then pandas exposed various properties and methos through **`str` accessor**. 

```
SeriesObj.str.operatorFunction()
``` 

**Available Functions**

Nearly all Python's built-in string methods are mirrored by a Pandas vectorized string method. Here is a list of Pandas str methods that mirror Python string methods:  

len()	lower()	translate()	islower()
ljust()	upper()	startswith()	isupper()
rjust()	find()	endswith()	isnumeric()
center()	rfind()	isalnum()	isdecimal()
zfill()	index()	isalpha()	split()
strip()	rindex()	isdigit()	rsplit()
rstrip()	capitalize()	isspace()	partition()
lstrip()	swapcase()	istitle()	rpartition()

#### Regex Extractor

Extract capture **groups** in the regex pattern, by default in DataFrame (`expand=True`).

```
Series.str.extract(self, pat, flags=0, expand=True)
- expand=True: if result is single column, make it a Series instead of Dataframe.
```

```{python}
s = pd.Series(['a1', 'b2', 'c3'])
print( 
  ' Extracted Dataframe:\n', s.str.extract(r'([ab])(\d)'),'\n\n',
  'Extracted Dataframe witn Names:\n', s.str.extract(r'(?P<Letter>[ab])(\d)'))
```

Below ouptut single columne, use **`expand=False`** to make the result a **Series**, instead of DataFrame.

```{python}
r = s.str.extract(r'[ab](\d)', expand=False)
print( r, '\n\n', type(r) )
```
#### Character Extractor

```{python}
monte = pd.Series(['Graham Chapman', 'John Cleese', 'Terry Gilliam',
                   'Eric Idle', 'Terry Jones', 'Michael Palin'])
monte
```

**```startwith```**

```{python}
monte.str.startswith('T')
```

**```Slicing```**

```{python}
monte.str[0:3]
```

#### Splitting

Split strings around given separator/delimiter in either string or regex.

```
Series.str.split(self, pat=None, n=-1, expand=False)
- pat: can be string or regex
```

```{python}
s = pd.Series(['a_b_c', 'c_d_e', np.nan, 'f_g_h_i_j'])
s
```

**```str.split()```** by default, split will split each item into **array**

```{python}
s.str.split('_')
```

**```expand=True```** will return a **dataframe** instead of series. By default, expand split into all possible columns.

```{python}
print( s.str.split('_', expand=True) )
```

It is possible to limit the number of columns splitted

```{python}
print( s.str.split('_', expand=True, n=1) )
```

**```str.rsplit()```**


**```rsplit```** stands for **reverse split**, it works the same way, except it is reversed

```{python}
print( s.str.rsplit('_', expand=True, n=1) )
```


#### Case Conversion

```
SeriesObj.str.upper()
SeriesObj.str.lower()
SeriesObj.str.capitalize()
```

```{python}
s = pd.Series(['A', 'B', 'C', 'aAba', 'bBaca', np.nan, 'cCABA', 'dog', 'cat'])
print( s.str.upper(), '\n',
       s.str.capitalize())
```

#### Number of Characters

```{python}
s.str.len()
```

#### String Indexing

This return specified character from each item.

```{python}
s = pd.Series(['A', 'B', 'C', 'Aaba', 'Baca', np.nan,'CABA', 'dog', 'cat'])
s.str[0].values    # first char
s.str[0:2].values  # first and second char
```

#### Series Substring Extraction

**Sample Data**

```{python}
s = pd.Series(['a1', 'b2', 'c3'])
s
```


**Extract absed on regex matching**  
... to improve ...

```{python}
type(s.str.extract('([ab])(\d)', expand=False))
```


### Series DateTime Accessor ```.dt```

If the underlying data is **datetime64** type, then pandas exposed various properties and methos through **```dt``` accessor**. 


#### Sample Data

```{python}
s = pd.Series([
    datetime(2000,1,1,0,0,0),
    datetime(1999,12,15,12,34,55),
    datetime(2020,3,8,5,7,12),
    datetime(2018,1,1,0,0,0),
    datetime(2003,3,4,5,6,7)
])
s
```


#### Convert To 
**datetime.datetime**  
Use **```to_pydatetime()```** to convert into **```numpy.array```** of standard library **```datetime.datetime```**  

```{python}
pdt  = s.dt.to_pydatetime()
print( type(pdt) )
pdt
```


**datetime.date**  
Use **```dt.date```** to convert into **```pandas.Series```** of standard library **```datetime.date```**   
Is it possible to have a pandas.Series of datetime.datetime ? No, because Pandas want it as its own Timestamp.

```{python}
sdt = s.dt.date
print( type(sdt[1] ))
print( type(sdt))
sdt
```


#### Timestamp Attributes
A Series::DateTime object support below properties:  
- date  
- month  
- day  
- year  
- dayofweek  
- dayofyear  
- weekday  
- weekday_name  
- quarter  
- daysinmonth  
- hour
- minute

Full list below:  
https://pandas.pydata.org/pandas-docs/stable/reference/series.html#datetimelike-properties

```{python}
s.dt.date
```

```{python}
s.dt.month
```

```{python}
s.dt.dayofweek
```

```{python}
s.dt.weekday
```

```{python}
s.dt.weekday_name
```

```{python}
s.dt.quarter
```

```{python}
s.dt.daysinmonth
```

```{python}
s.dt.time   # extract time as time Object
```

```{python}
s.dt.hour  # extract hour as integer
```

```{python}
s.dt.minute # extract minute as integer
```


## class: DataFrame

### Constructor

#### From Row Oriented Data (List of Lists)
Create from **List of Lists**
```
DataFrame( [row_list1, row_list2, row_list3] )
DataFrame( [row_list1, row_list2, row_list3], column=columnName_list )
DataFrame( [row_list1, row_list2, row_list3], index=row_label_list )
```


**Basic DataFrame with default Row Label and Column Header**

```{python}
pd.DataFrame ([[101,'Alice',40000,2017],
               [102,'Bob',  24000, 2017], 
               [103,'Charles',31000,2017]] )
```


**Specify Column Header during Creation**

```{python}
pd.DataFrame ([[101,'Alice',40000,2017],
               [102,'Bob',  24000, 2017], 
               [103,'Charles',31000,2017]], columns = ['empID','name','salary','year'])
```


**Specify Row Label during Creation**

```{python}
pd.DataFrame ([[101,'Alice',40000,2017],
               [102,'Bob',  24000, 2017], 
               [103,'Charles',31000,2017]], index   = ['r1','r2','r3'] )
```


#### From Row Oriented Data (List of Dictionary)
```
DataFrame( [dict1, dict2, dict3] )
DataFrame( [row_list1, row_list2, row_list3], column=np.arrange )
DataFrame( [row_list1, row_list2, row_list3], index=row_label_list )

by default,keys will become collumn names, and autosorted
```


**Default Column Name Follow Dictionary Key**  
Note missing info as NaN

```{python}
pd.DataFrame ([{"name":"Yong", "id":1,"zkey":101},{"name":"Gan","id":2}])
```


**Specify Index**

```{python}
pd.DataFrame ([{"name":"Yong", "id":'wd1'},{"name":"Gan","id":'wd2'}], 
             index = (1,2))
```


**Specify Column Header during Creation**, can acts as column filter and manual arrangement  
Note missing info as NaN

```{python}
pd.DataFrame ([{"name":"Yong", "id":1, "zkey":101},{"name":"Gan","id":2}], 
              columns=("name","id","zkey"))
```


#### From Column Oriented Data
Create from **Dictrionary of List**
```
DataFrame(  { 'column1': list1,
              'column2': list2,
              'column3': list3 } , 
              index    = row_label_list, 
              columns  = column_list)
              
```
By default, DataFrame will **arrange the columns alphabetically**, unless **columns** is specified


**Default Row Label**

```{python}
data = {'empID':  [100,      101,    102,      103,     104],
        'year':   [2017,     2017,   2017,      2018,    2018],
        'salary': [40000,    24000,  31000,     20000,   30000],
        'name':   ['Alice', 'Bob',  'Charles', 'David', 'Eric']}
pd.DataFrame(data)
```


**Specify Row Label during Creation**

```{python}
data = {'empID':  [100,      101,    102,      103,     104],
        'name':   ['Alice', 'Bob',  'Charles', 'David', 'Eric'],
        'year':   [2017,     2017,   2017,      2018,    2018],
        'salary': [40000,    24000,  31000,     20000,   30000] }
pd.DataFrame (data, index=['r1','r2','r3','r4','r5'])
```


**Manualy Choose Columns and Arrangement**

```{python}
data = {'empID':  [100,      101,    102,      103,     104],
        'name':   ['Alice', 'Bob',  'Charles', 'David', 'Eric'],
        'year':   [2017,     2017,   2017,      2018,    2018],
        'salary': [40000,    24000,  31000,     20000,   30000] }
pd.DataFrame (data, columns=('empID','name','salary'), index=['r1','r2','r3','r4','r5'])
```


### Attributes

```{python}
df = pd.DataFrame(
    { 'empID':  [100,      101,    102,      103,     104],
      'year1':   [2017,     2017,   2017,      2018,    2018],
      'name':   ['Alice',  'Bob',  'Charles','David', 'Eric'],
      'year2':   [2001,     1907,   2003,      1998,    2011],
      'salary': [40000,    24000,  31000,     20000,   30000]},
    columns = ['year1','salary','year2','empID','name'])
```


#### Dimensions

```{python}
df.shape
```


#### Index

```{python}
df.index
```

**Underlying Index values are numpy object**

```{python}
df.index.values
```

#### Columns

```{python}
df.columns
```

**Underlying Index values are numpy object**

```{python}
df.columns.values
```

#### Values

**Underlying Column values are numpy object**

```{python}
df.values
```

### Index Manipulation
**index** and **row label** are used interchangeably in this book


#### Sample Data
Columns are intentionaly ordered in a messy way 

```{python}
df = pd.DataFrame(
    { 'empID':  [100,      101,    102,      103,     104],
      'year1':   [2017,     2017,   2017,      2018,    2018],
      'name':   ['Alice',  'Bob',  'Charles','David', 'Eric'],
      'year2':   [2001,     1907,   2003,      1998,    2011],
      'salary': [40000,    24000,  31000,     20000,   30000]},
    columns = ['year1','salary','year2','empID','name'])

print (df, '\n')
print (df.index)
```

#### Convert Column To Index
```
set_index('column_name', inplace=False)
```
**inplace=True** means don't create a new dataframe. Modify existing dataframe    
**inplace=False** means return a new dataframe

```{python}
print(df)
print(df.index,'\n')

df.set_index('empID',inplace=True) 
print(df)
print(df.index) # return new DataFrameObj
```

#### Convert Index Back To Column
- Reseting index will resequence the index as 0,1,2 etc  
- Old index column will be converted back as normal column  
- Operation support inplace** option

```{python}
df.reset_index(inplace=True)
print(df)
```


#### Updating Index ( .index= )
**Warning:**  
- Updating index doesn't reorder the data sequence  
- Number of elements before and after reorder must match, otherwise **error**  
- Same label are **allowed to repeat**
- Not reversable

```{python}
df.index = [101, 101, 101, 102, 103]
print( df )
```


#### Reordering Index (. reindex )
- Reindex will reorder the rows according to new index  
- The operation is not reversable


**Start from this original dataframe**


**Change the order of Index**, always return a new dataframe

```{python}
df.index = [101,102,103,104,105]
print( df )                                ## original sequence
print( df.reindex([103,102,101,104,105]) ) ## new sequence, new dataframe
```


### Subsetting Columns
**Select Single Column** Return **Series**
```
dataframe.columnName               # single column, name based, return Series object
dataframe[ single_col_name ]       # single column, name based, return Series object
dataframe[ [single_col_name] ]     # single column, name based, return DataFrame object
```
**Select Single/Multiple Columns** Return **DataFrame**
```
dataframe[ single/list_of_col_names ]                       # name based, return Dataframe object
dataframe.loc[ : , single_col_name  ]  # single column, series
dataframe.loc[ : , col_name_list    ]  # multiple columns, dataframe
dataframe.loc[ : , col_name_ranage  ]  # multiple columns, dataframe

dataframe.iloc[ : , col_number      ]  # single column, series
dataframe.iloc[ : , col_number_list ]  # multiple columns, dataframe
dataframe.iloc[ : , number_range    ]  # multiple columns, dataframe
```


#### Select Single Column
Selecting single column always return as ```panda::Series```

```{python}
df.name
```

```{python}
df['name']
```

```{python}
df.loc[:, 'name']
```

```{python}
df.iloc[:, 3]
```


#### Select Multiple Columns
Multiple columns return as **panda::Dataframe** object`

```{python}
df[['name']]  # return one column dataframe
```

```{python}
print(df.columns)
df[['name','year1']]
```

```{python}
df.loc[:,['name','year1']]
```

```{python}
df.loc[:,'year1':'year2']  # range of columns
```

```{python}
df.iloc[:,[0,3]]
```

```{python}
df.iloc[:,0:3]
```


#### Selection by Data Type

```
df.select_dtypes(include=None, exclude=None)
```
Always return **panda::DataFrame**, even though only single column matches.  
Allowed types are:
- number (integer and float)  
- integer / float 
- datetime  
- timedelta  
- category  

```{python}
df.get_dtype_counts()
```

```{python}
df.select_dtypes(exclude='number')
```

```{python}
df.select_dtypes(exclude=('number','object'))
```


#### Subset by ```filter()```
```.filter(items=None, like=None, regex=None, axis=1)```  


**like = Substring Matches**  

```{python}
df.filter( like='year',  axis='columns')  ## or axis = 1
```


**items = list of column names**

```{python}
df.filter( items=('year1','year2'),  axis=1)  ## or axis = 1
```


**regex = Regular Expression**  
Select column names that contain integer

```{python}
df.filter(regex='\d')  ## default axis=1 if DataFrame
```


### Column Manipulation


#### Sample Data

```{python}
df
```


#### Renaming Columns


**Method 1 : Rename All Columns (.columns =)**  
- Construct the new column names, **check if there is no missing** column names   
- **Missing columns** will return **error**  
- Direct Assignment to column property result in change to dataframe

```{python}
new_columns = ['year.1','salary','year.2','empID','name']
df.columns = new_columns
df.head(2)
```


**Method 2 : Renaming Specific Column (.rename (columns=) )** 
- Change column name through **rename** function  
- Support **inpalce** option for original dataframe change  
- Missing column is OK

```{python}
df.rename( columns={'year.1':'year1', 'year.2':'year2'}, inplace=True)
df.head(2)
```


#### Reordering Columns
Always return a new dataframe.  There is **no inplace option** for reordering columns  

**Method 1 - reindex(columns = )**  
- **reindex** may sounds like operation on row labels, but it works  
- **Missmatch** column names will result in **NA** for the unfound column

```{python}
new_colorder = [ 'empID', 'name', 'salary', 'year1', 'year2']
df.reindex(columns = new_colorder).head(2)
```


**Method 2 - [ ] notation**  
- **Missmatch** column will result in **ERROR**  

```{python}
new_colorder = [ 'empID', 'name', 'salary', 'year1', 'year2']
df[new_colorder]
```


#### Duplicating or Replacing Column
- **New Column** will be created instantly using **[] notation**  
- **DO NOT USE dot Notation** because it is view only attribute

```{python}
df['year3'] = df.year1
df
```


#### Dropping Columns (.drop)
```
dataframe.drop( columns='column_name',    inplace=True/False)   # delete single column
dataframe.drop( columns=list_of_colnames, inplace=True/False)   # delete multiple column

dataframe.drop( index='row_label',         inplace=True/False)   # delete single row
dataframe.drop( index= list_of_row_labels, inplace=True/False)   # delete multiple rows

```
**inplace=True** means column will be deleted from original dataframe. **Default is False**, which return a copy of dataframe  


**By Column Name(s)**

```{python}
df.drop( columns='year1') # drop single column
```

```{python}
df.drop(columns=['year2','year3'])  # drop multiple columns
```


**By Column Number(s)**   
Use dataframe.columns to produce interim list of column names

```{python}
df.drop( columns=df.columns[[3,4,5]] )   # delete columns by list of column number
```

```{python}
df.drop( columns=df.columns[3:6] )       # delete columns by range of column number
```


### Subsetting Rows
```
dataframe.loc[ row_label       ]  # return series, single row
dataframe.loc[ row_label_list  ]  # multiple rows
dataframe.loc[ boolean_list    ]  # multiple rows

dataframe.iloc[ row_number       ]  # return series, single row
dataframe.iloc[ row_number_list  ]  # multiple rows
dataframe.iloc[ number_range     ]  # multiple rows

dataframe.sample(frac=)                                        # frac = 0.6 means sampling 60% of rows randomly
```


#### Sample Data

```{python}
df = pd.DataFrame(
    { 'empID':  [100,      101,    102,      103,     104],
      'year1':   [2017,     2017,   2017,      2018,    2018],
      'name':   ['Alice',  'Bob',  'Charles','David', 'Eric'],
      'year2':   [2001,     1907,   2003,      1998,    2011],
      'salary': [40000,    24000,  31000,     20000,   30000]},
    columns = ['year1','salary','year2','empID','name']).set_index(['empID'])
df
```


#### By Index or Boolean


**Single Index** return Series


```{python}
df.loc[101]         # by single row label, return series
```


**List or Range of Indexes** returns DataFrame

```{python}
df.loc[ [100,103] ]  # by multiple row labels
```

```{python}
df.loc[  100:103  ]  # by range of row labels
```


**List of Boolean** returns DataFrame

```{python}
criteria = (df.salary > 30000) & (df.year1==2017)
print (criteria)
print (df.loc[criteria])
```


#### By Row Number
**Single Row** return Series

```{python}
df.iloc[1]  # by single row number
```


Multiple rows **returned as dataframe** object

```{python}
df.iloc[ [0,3] ]    # by row numbers
```

```{python}
df.iloc[  0:3  ]    # by row number range
```


#### ```query()```
```.query(expr, inplace=False)```

```{python}
df.query('salary<=31000 and year1 == 2017')
```


#### ```sample()```

```{python}
np.random.seed(15)
df.sample(frac=0.6) #randomly pick 60% of rows, without replacement
```


### Row Manipulation

#### Sample Data

#### Appending Rows

Appending rows is more computaional intensive then concatenate.
Item can be added as single item or multi-items (list form)

**Append From Another DataFrame**

- When `ignore_index=True`, pandas will **drop the original Index** and recreate with 0,1,2,3...  
- It is recommended to ignore index IF the data source index is **not unique**.  
- New columns will be added in the result, with NaN on original dataframe.   

```{python}
my_df = pd.DataFrame(
          data= {'Id':   [10,20,30],
                 'Name': ['Aaa','Bbb','Ccc']})
#                 .set_index('Id')
                 
my_df_new = pd.DataFrame(
            data= {'Id':   [40,50],
                   'Name': ['Ddd','Eee'],
                   'Age':  [12,13]})  
                   #.set_index('Id')
                   
my_df_append  = my_df.append(my_df_new, ignore_index=False)
my_df_noindex = my_df.append(my_df_new, ignore_index=True)

print("Original DataFrame:\n", my_df,
      "\n\nTo Be Appended DataFrame:\n", my_df_new,
      "\n\nAppended DataFrame (index maintained):\n", my_df_append,
      "\n\nAppended DataFrame (index ignored):\n", my_df_noindex)
```

**Append From Dictionary**

```{python}
my_df = pd.DataFrame(
          data= {'Id':   [10,20,30],
                 'Name': ['Aaa','Bbb','Ccc']})  \
                 .set_index('Id')

new_item1 = {'Id':40, 'Name': 'Ddd'}
new_item2 = {'Id':50, 'Name': 'Eee'}
new_item3 = {'Id':60, 'Name': 'Fff'}

my_df_one   = my_df.append( new_item1, ignore_index=True )
my_df_multi = my_df.append( [new_item2, new_item3], ignore_index=True )

print("Original DataFrame:\n", my_df,
      "\n\nAdd One Item (index ignored):\n", my_df_one,
      "\n\nAdd Multi Item (index ignored):\n", my_df_multi)
```


**Appending `None` items(s)**

Adding **single None** item has **no effect** (nothing added).   
Adding **None in list form (multiple items)**  creates rows with None.  
`ignore_index` is not important here.

```{python}
single_none = my_df.append( None  )
multi_none  = my_df.append( [None])

print("Original DataFrame:\n", my_df,
      "\n\nAdd One None (index ignored):\n", single_none,
      "\n\nAdd List of None (index ignored):\n", multi_none)
```

**Appending Items Containing None** results in **ERROR**

```{python}
my_df.append( [new_item1, None] )
```


#### Concatenate Rows

```{python}

```



#### Dropping Rows (.drop)
```.drop(labels=None, axis=0, index=None, columns=None, level=None, inplace=False, errors='raise')```

**By Row Label(s)**

```{python}
df.drop(index=100)       # single row
```

```{python}
df.drop(index=[100,103])   # multiple rows
```


### Slicing


#### Sample Data

```{python}
df
```


#### Getting One Cell  
**By Row Label and Column Name (loc)**


```
dataframe.loc [ row_label , col_name   ]    # by row label and column names
dataframe.loc [ bool_list , col_name   ]    # by row label and column names
dataframe.iloc[ row_number, col_number ]    # by row and column number
```

```{python}
print (df.loc[100,'year1'])
```


**By Row Number and Column Number (iloc)**

```{python}
print (df.iloc[1,2])
```


#### Getting Multiple Cells
Specify rows and columns (by individual or range)

```
dataframe.loc [ list/range_of_row_labels , list/range_col_names   ]    # by row label and column names
dataframe.iloc[ list/range_row_numbers,    list/range_col_numbers ]    # by row number
```


**By Index and Column Name (loc)**

```{python}
print (df.loc[ [101,103], ['name','year1'] ], '\n')  # by list of row label and column names
print (df.loc[  101:104 ,  'year1':'year2'  ], '\n')  # by range of row label and column names
```


**By Boolean Row and Column Names (loc)**

```{python}
df.loc[df.year1==2017, 'year1':'year2']
```


**By Row and Column Number (iloc)**

```{python}
print (df.iloc[ [1,4], [0,3]],'\n' )   # by individual rows/columns
print (df.iloc[  1:4 ,  0:3], '\n')    # by range
```


### Chained Indexing


**Chained Index** Method creates a copy of dataframe, any modification of data on original dataframe does not affect the copy  
```
dataframe.loc  [...]  [...]
dataframe.iloc [...]  [...]
```
Suggesting, **never use** chain indexing

```{python}
df = pd.DataFrame(
    { 'empID':  [100,      101,    102,      103,     104],
      'year1':   [2017,     2017,   2017,      2018,    2018],
      'name':   ['Alice',  'Bob',  'Charles','David', 'Eric'],
      'year2':   [2001,     1907,   2003,      1998,    2011],
      'salary': [40000,    24000,  31000,     20000,   30000]},
    columns = ['year1','salary','year2','empID','name']).set_index(['empID'])
df
```

```{python}
df.loc[100]['year'] =2000
df  ## notice row label 100 had not been updated, because data was updated on a copy due to chain indexing
```


### Iteration

#### `.iterrows()` loop through every **ROW**

```{python}
df = pd.DataFrame(data=
    { 'empID':  [100,      101,    102,      103,     104],
      'Name':   ['Alice',  'Bob',  'Charles','David', 'Eric'],
      'Year':   [1999,     1988,   2001,     2010,     2020]}).set_index(['empID'])

for idx, row in df.iterrows():
  print(idx, row.Name)
```

#### `.items()` loop through every **Column**

```{python}
for label, content in df.items():
  print('Label:',            label,   '\n\n', 
        'Content (Series):\n', content, '\n\n')
```

### Data Structure


#### Instance Methods - Structure
Find out the column names, data type in a summary. Output is for display only, not a data object

```{python}
df.info()  # return text output
```

```{python}
df.get_dtype_counts() # return Series
```


#### Conversion To Other Format

```{python}
df.to_json()
```

```{python}
df.to_records()
```

```{python}
df.to_csv()
```


### Exploratory Analysis


#### Sample Data

```{python}
df
```


#### All Stats in One  - .describe()


```
df.describe(include='number') # default
df.describe(include='object') # display for non-numeric columns
df.describe(include='all')    # display both numeric and non-numeric
```

When applied to DataFrame object, describe shows all **basic statistic** for **all numeric** columns:
- Count (non-NA)  
- Unique (for string)  
- Top (for string)   
- Frequency (for string)  
- Percentile  
- Mean  
- Min / Max  
- Standard Deviation  


**For Numeric Columns only**  
You can **customize the percentiles requred**. Notice 0.5 percentile is always there although not specified

```{python}
df.describe()
```

```{python}
df.describe(percentiles=[0.9,0.3,0.2,0.1])
```


**For both Numeric and Object**

```{python}
df.describe(include='all')
```


#### min/max/mean/median

```{python}
df.min()  # default axis=0, column-wise
```

```{python}
df.min(axis=1) # axis=1, row-wise
```


Observe, sum on **string will concatenate column-wise**, whereas row-wise only sum up numeric fields

```{python}
df.sum(0)
```

```{python}
df.sum(1)
```


### Plotting

```{python}

```

```{python}

```


## class: Categories

### Creating

#### From List
**Basic (Auto Category Mapping)**  
Basic syntax return categorical index with sequence with code 0,1,2,3... mapping to first found category   
In this case, **low(0), high(1), medium(2)**

```{python}
temp = ['low','high','medium','high','high','low','medium','medium','high']
temp_cat = pd.Categorical(temp)
temp_cat
```

```{python}
type( temp_cat )
```


**Manual Category Mapping**  
During creation, we can specify mapping of codes to category: **low(0), medium(1), high(2)**

```{python}
temp_cat = pd.Categorical(temp, categories=['low','medium','high'])
temp_cat
```


#### From Series
- We can 'add' categorical structure into a Series. With these methods, additional property (.cat) is added as a **categorical accessor**  
- Through this accessor, you gain access to various properties of the category such as .codes, .categories. But not .get_values() as the information is in the Series itself  
- Can we manual map category ?????

```{python}
temp = ['low','high','medium','high','high','low','medium','medium','high']
temp_cat = pd.Series(temp, dtype='category')
print (type(temp_cat))       # Series object
print (type(temp_cat.cat))   # Categorical Accessor
```


- Method below has the same result as above by using **.astype('category')**  
- It is useful adding category structure into existing series.

```{python}
temp_ser = pd.Series(temp)
temp_cat = pd.Series(temp).astype('category')
print (type(temp_cat))       # Series object
print (type(temp_cat.cat))   # Categorical Accessor
```

```{python}
temp_cat.cat.categories
```


#### Ordering Category

```{python}
temp = ['low','high','medium','high','high','low','medium','medium','high']
temp_cat = pd.Categorical(temp, categories=['low','medium','high'], ordered=True)
temp_cat
```

```{python}
temp_cat.get_values()
```

```{python}
temp_cat.codes
```

```{python}
temp_cat[0] < temp_cat[3]
```

<!-- jupyter_markdown, jupyter_meta = list(heading_collapsed = TRUE) -->
### Properties

<!-- jupyter_markdown, jupyter_meta = list(hidden = TRUE) -->
#### .categories
first element's code = 0  
second element's code = 1  
third element's code = 2

```{python, jupyter_meta = list(hidden = TRUE)}
temp_cat.categories
```

<!-- jupyter_markdown, jupyter_meta = list(hidden = TRUE) -->
#### .codes
Codes are actual **integer** value stored as array. 1 represent 'high', 

```{python, jupyter_meta = list(hidden = TRUE)}
temp_cat.codes
```

<!-- jupyter_markdown, jupyter_meta = list(heading_collapsed = TRUE) -->
### Rename Category

<!-- jupyter_markdown, jupyter_meta = list(hidden = TRUE) -->
#### Renamce To New Category Object
**.rename_categories()** method return a new category object with new changed categories

```{python, jupyter_meta = list(hidden = TRUE)}
temp = ['low','high','medium','high','high','low','medium','medium','high']
new_temp_cat = temp_cat.rename_categories(['sejuk','sederhana','panas'])
new_temp_cat 
```

```{python, jupyter_meta = list(hidden = TRUE)}
temp_cat   # original category object categories not changed
```

<!-- jupyter_markdown, jupyter_meta = list(hidden = TRUE) -->
#### Rename Inplace
Observe the original categories had been changed using **.rename()**

```{python, jupyter_meta = list(hidden = TRUE)}
temp_cat.categories = ['sejuk','sederhana','panas']
temp_cat   # original category object categories is changed
```

<!-- jupyter_markdown, jupyter_meta = list(heading_collapsed = TRUE) -->
### Adding New Category
This return a new category object with added categories

```{python, jupyter_meta = list(hidden = TRUE)}
temp_cat_more = temp_cat.add_categories(['susah','senang'])
temp_cat_more
```

<!-- jupyter_markdown, jupyter_meta = list(heading_collapsed = TRUE) -->
### Removing Category
This is **not in place**, hence return a new categorical object  

<!-- jupyter_markdown, jupyter_meta = list(hidden = TRUE) -->
#### Remove Specific Categor(ies)
Elements with its category removed will become **NaN**

```{python, jupyter_meta = list(hidden = TRUE)}
temp = ['low','high','medium','high','high','low','medium','medium','high']
temp_cat = pd.Categorical(temp)
temp_cat_removed = temp_cat.remove_categories('low')
temp_cat_removed
```

<!-- jupyter_markdown, jupyter_meta = list(hidden = TRUE) -->
#### Remove Unused Category
Since categories removed are not used, there is no impact to the element

```{python, jupyter_meta = list(hidden = TRUE)}
print (temp_cat_more)
temp_cat_more.remove_unused_categories()
```

<!-- jupyter_markdown, jupyter_meta = list(heading_collapsed = TRUE) -->
### Add and Remove Categories In One Step - Set()

```{python, jupyter_meta = list(hidden = TRUE)}
temp = ['low','high','medium','high','high','low','medium','medium','high']
temp_cat = pd.Categorical(temp, ordered=True)
temp_cat
```

```{python, jupyter_meta = list(hidden = TRUE)}
temp_cat.set_categories(['low','medium','sederhana','susah','senang'])
```

<!-- jupyter_markdown, jupyter_meta = list(heading_collapsed = TRUE) -->
### Categorical Descriptive Analysis 

<!-- jupyter_markdown, jupyter_meta = list(hidden = TRUE) -->
#### At One Glance

```{python, jupyter_meta = list(hidden = TRUE)}
temp_cat.describe()
```

<!-- jupyter_markdown, jupyter_meta = list(hidden = TRUE) -->
#### Frequency Count

```{python, jupyter_meta = list(hidden = TRUE, scrolled = TRUE)}
temp_cat.value_counts()
```

<!-- jupyter_markdown, jupyter_meta = list(hidden = TRUE) -->
#### Least Frequent Category, Most Frequent Category, and Most Frequent Category

```{python, jupyter_meta = list(hidden = TRUE)}
( temp_cat.min(), temp_cat.max(), temp_cat.mode() )
```


### Other Methods


#### .get_values()
Since actual value stored by categorical object are integer **codes**, get_values() function return values translated from *.codes** property

```{python}
temp_cat.get_values()  #array
```


## Dummies

- **get_dummies** creates columns for each categories 
- The underlying data can be string or pd.Categorical  
- It produces a **new pd.DataFrame**


### Sample Data

```{python}
df = pd.DataFrame (
    {'A': ['A1', 'A2', 'A3','A1','A3','A1'], 
     'B': ['B1','B2','B3','B1','B1','B3'],
     'C': ['C1','C2','C3','C1',np.nan,np.nan]})
df
```


### Dummies on Array-Like Data

```{python}
pd.get_dummies(df.A)
```


### Dummies on DataFrame (multiple columns)


#### All Columns

```{python}
pd.get_dummies(df)
```


#### Selected Columns

```{python}
cols = ['A','B']
pd.get_dummies(df[cols])
```


### Dummies with na
By default, nan values are ignored

```{python}
pd.get_dummies(df.C)
```


**Make NaN as a dummy variable**

```{python}
pd.get_dummies(df.C,dummy_na=True)
```


### Specify Prefixes

```{python}
pd.get_dummies(df.A, prefix='col')
```

```{python}
pd.get_dummies(df[cols], prefix=['colA','colB'])
```


### Dropping First Column
- Dummies cause **colinearity issue** for regression as it has redundant column.  
- Dropping a column **does not loose any information** technically

```{python}
pd.get_dummies(df[cols],drop_first=True)
```




## GroupBy
- Aggretation and summarization require creating **DataFrameGroupBy** object from existing DataFrame  
- The **GroupBy** object is a **very flexible abstraction**. In many ways, you can simply treat it as if it's a **collection of DataFrames**, and it does the difficult things under the hood  

```{python}
company = pd.read_csv('data/company.csv')
company.head()
```


### Creating Groups

```{python}
com_grp = company.groupby(['Company','Department'])
com_grp
```

### Properties

#### Number of Groups Created

```{python}
com_grp.ngroups
```


#### Row Numbers Associated For Each Group

```{python}
com_grp.groups  # return Dictionary
```


### Methods

#### Number of Rows In Each Group

```{python}
com_grp.size()  # return panda Series object
```


#### Valid (not Null) Data Count For Each Fields In The Group

```{python}
com_grp.count()  # return panda DataFrame object
```


### Retrieve Rows
All row retrieval operations **return a dataframe**


#### Retrieve N Rows For Each Groups
Example below retrieve 2 rows from each group

```{python}
com_grp.head(2)
```


#### Retrieve Rows In One Specific Group

```{python}
com_grp.get_group(('C1','D3'))
```


#### Retrieve n-th Row From Each Group
Row number is 0-based

```{python}
com_grp.nth(-1)    # retireve last row from each group
```


### Iteration
**DataFrameGroupBy** object can be thought as a collection of named groups

```{python}
def print_groups (g):
    for name,group in g:
        print (name)
        print (group[:2])
        
print_groups (com_grp)
```

```{python}
com_grp
```


### Apply Aggregate Functions to Groups
Aggregate apply functions to columns in every groups, and return a summary data for each group


#### Apply One Function to One or More Columns

```{python}
com_grp['Age'].sum()
```

```{python}
com_grp[['Age','Salary']].sum()
```


#### Apply One or More Functions To All Columns

```{python}
com_grp.agg(np.mean)
```

```{python}
com_grp.agg([np.mean,np.sum])
```


#### Apply Different Functions To Different Columns

```{python}
com_grp.agg({'Age':np.mean, 'Salary': [np.min,np.max]})
```


### Transform


- Transform is an operation used combined with **DataFrameGroupBy** object  
- **transform()** return a **new DataFrame object**  

```{python}
grp = company.groupby('Company')
grp.size()
```


**transform()** perform a function to a group, and **expands and replicate** it to multiple rows according to original DataFrame

```{python}
grp[['Age','Salary']].transform('sum')
```

```{python}
grp.transform( lambda x:x+10 )
```





## Fundamental Analysis

## Missing Data

### What Is Considered Missing Data ? 

### Sample Data

```{python}
df = pd.DataFrame( np.random.randn(5, 3), 
                   index   =['a', 'c', 'e', 'f', 'h'],
                   columns =['one', 'two', 'three'])
df['four'] = 'bar'
df['five'] = df['one'] > 0
#df
df.reindex(['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h'])
```

**How Missing Data For Each Column ?**

```{python}
df.count()
```

```{python}
len(df.index) - df.count()
```

```{python}
df.isnull()
```

```{python}
df.describe()
```


